<!DOCTYPE HTML>
<html>

<head>
	<title>webcam-eyetracking</title>
	<script src="https://sdk.amazonaws.com/js/aws-sdk-2.7.16.min.js"></script>
	<!--Load jQuery-->
	<script src="./assets/js/jquery-1.12.3.min.js"></script>
	<script src="./assets/js/jquery-ui.js"></script>
	<!--Stylesheet-->
	<link rel="stylesheet" type="text/css" href="./assets/css/medusa.css">
	<!--Script-->
	<script src="./assets/js/settings.js" type="text/javascript"></script>
	<script src="./assets/js/medusa.js" type="text/javascript"></script>
	<script src="./assets/js/heatmap.js" type="text/javascript"></script>

</head>

<body>
	<!-- Main -->
	<div id="main">
		<div class="inner">
			<h1>Can we use your webcam to guess where you're looking?</h1>
			<p style="font-weight: 400;"> Hi! </p>
			<p style="font-weight: 400;">We are researchers at Bucknell University
				<a href="mailto:emp017@bucknell.edu" target="_blank"> [emp017@bucknell.edu]</a> who are trying to understand how people look at information online. This work could help us
				redesign our surveys, articles, and educational material in a way that fits people better. </p>
			<p style="font-weight: 400;"> To do that, we are exploring how your webcam can be used to infer where you are looking on the screen. Don't worry - we
				won't see ANY images of you (in fact, they never leave your computer). While you are on our site, we'll be sent ONLY
				the coordinates of where you're looking on our site (and NO other identifying info). At the end, you'll fill out a very
				quick survey to help us understand you a little more. </p>
			<p style="font-weight: 400;"> If you get to the end of our experiment (about 10 minutes), we'll also let you play with a creative demo of this technology
				- everywhere you look on the screen will start to become colored! After you "paint a picture with your eyes", we built
				in some ways for you to share that painting with your friends (or just keep it yourself!). </p>
			<p style="font-weight: 400;"> Thank you for visiting and we hope you'll contribute to our research! </p>
			<p style="font-weight: 400;"> For more details, go to the next screen.... </p>
			<iframe src="https://giphy.com/embed/YXsTqtJtnGUGKGemZ4" width="480" height="320" style="border: 1px solid black;margin:auto; padding:auto"
			    frameBorder="10" class="giphy-embed" allowFullScreen></iframe>
			<p style="font-weight: 400;">
				The core eye-tracking technology comes from the
				<a href=" https://webgazer.cs.brown.edu/">Webga zer project at Brown University</a>.
			</p>
			<p style="font-weight: 400;">The stimuli is based on the
				<a href=" http://massvis.mit.edu/">MassVis Dataset - one of the largest real-world visualization databases.</a>
			</p>
			<p style="font-weight: 400;">
				The experimental platform builds on the
				<a href=" https://osf.io/jmz79/">research paper by Kilian Semmelmann and Sarah Weigelt. </a>
			</p>
			<a href="#" class="form__button" onclick="start_medusa('simple')">Continue</a>
		</div>
	</div>
</body>
<!-- <script>start_medusa('simple')</script> -->

</html>